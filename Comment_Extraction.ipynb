{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348e893b-e773-4e96-956d-74e6fd110ef4",
   "metadata": {},
   "source": [
    "# Download an archive\n",
    "The script below makes a data folder with a subfolder for the dataset. Dataset option are \"World\", \"German\" or \"Dutch\", select these by altering the ``dataset`` variable. The script wil skip files that are already present on the system. If you want to redownload the files first remove the old files manually.  \n",
    "If the download fails due to connection errors, rerun the script and will will notice that he file did not decompress and try again.  \n",
    "Note: The html-file-information.csv can be very large and take several hours and fail. Consider putting the code below code in a loop if you want to run it over night, if the download fails it will try again and if successfull it will say it already has the desired file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77006d6e-6e30-43b3-9353-37de21a7e004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "mkdir: cannot create directory ‘data/World’: File exists\n",
      "domain-frequency.csv already exists\n",
      "domain-frequency.csv already exists\n",
      "css-file-information.csv already exists\n",
      "js-file-information.csv already exists\n",
      "html-file-information.csv already exists\n"
     ]
    }
   ],
   "source": [
    "# Dataset option are \"World\", \"German\" or \"Dutch\"\n",
    "Dataset = \"World\"\n",
    "\n",
    "!mkdir data\n",
    "!mkdir data/{Dataset}\n",
    "\n",
    "import os.path\n",
    "\n",
    "if Dataset == \"German\":\n",
    "    if not os.path.exists(\"data/German/domain-frequency.csv\"):\n",
    "        print(\"domain-frequency.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/DomainFrequencyExtraction/domain-frequency.csv.gz?access=VRQ4COI5RFEB6XTJZNQTBRLEZTTHJERL\" --output data/German/domain-frequency.csv.gz\n",
    "        print(\"domain-frequency.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/German/domain-graph.csv\"):\n",
    "        print(\"domain-graph.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/DomainGraphExtraction/domain-graph.csv.gz?access=JKEPGQ6MUC72JQB23IXOC4KOLGJYDSMN\" --output data/German/domain-graph.csv.gz\n",
    "        print(\"domain-graph.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/German/css-file-information.csv\"):\n",
    "        print(\"css-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/css-file-information.csv.gz?access=I2WP4REJA3NOBU3TCAAL3OIGJKNXM46R\" --output data/German/css-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"css-file-information.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/German/js-file-information.csv\"):\n",
    "        print(\"js-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/js-file-information.csv.gz?access=M3QSMFPLEHPZPWZIFSMZ6CT2OO7WYQ4M\" --output data/German/js-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"js-file-information.csv already exists\")\n",
    "        \n",
    "    if not os.path.exists(\"data/German/html-file-information.csv\"):\n",
    "        print(\"html-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/html-file-information.csv.gz?access=E3BUHKL5P4Q3TD4LGXZV2AOMERYQ3GWL\" --output data/German/html-file-information.csv.gz\n",
    "        print(\"html-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"html-file-information.csv already exists\")\n",
    "        \n",
    "if Dataset == \"World\":\n",
    "    if not os.path.exists(\"data/World/domain-frequency.csv\"):\n",
    "        print(\"domain-frequency.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/DomainFrequencyExtraction/domain-frequency.csv.gz?access=SMSQY3G6IGKGRWVLGCWMA7DMCHBCKQ4K\" --output data/World/domain-frequency.csv.gz\n",
    "        print(\"domain-frequency.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/World/domain-graph.csv\"):\n",
    "        print(\"domain-graph.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/DomainGraphExtraction/domain-graph.csv.gz?sample=true&access=BZTA7LW5LUNMPGKWQMLKAUAXBV2E2AEC\" --output data/World/domain-graph.csv.gz\n",
    "        print(\"domain-graph.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/World/css-file-information.csv\"):\n",
    "        print(\"css-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/css-file-information.csv.gz?access=UZ4PWVRXXWPF53BHM7TTYNK24P7YAIXQ\" --output data/World/css-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"css-file-information.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/World/js-file-information.csv\"):\n",
    "        print(\"js-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/js-file-information.csv.gz?access=54X7IV7QOOAJWGHKPRGDI7HIR6W6GKPI\" --output data/World/js-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"js-file-information.csv already exists\")\n",
    "    \n",
    "    if not os.path.exists(\"data/World/html-file-information.csv\"):\n",
    "        print(\"html-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/html-file-information.csv.gz?access=JQEDT3PRXOS6OXZ7LA5EC55HQJVUASDX\" --output data/World/html-file-information.csv.gz\n",
    "        print(\"html-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"html-file-information.csv already exists\")\n",
    "\n",
    "if Dataset == \"Dutch\":\n",
    "    if not os.path.exists(\"data/Dutch/domain-frequency.csv\"):\n",
    "        print(\"domain-frequency.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/DomainFrequencyExtraction/domain-frequency.csv.gz?access=QUQTRHVNIDKXN62S4D62XFHCFI7VDSH7\" --output data/Dutch/domain-frequency.csv.gz\n",
    "        print(\"domain-frequency.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/Dutch/domain-graph.csv\"):\n",
    "        print(\"domain-graph.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/DomainGraphExtraction/domain-graph.csv.gz?access=JUNESOMDTBYDNCCGVJI5MJHW45KHAIJZ\" --output data/Dutch/domain-graph.csv.gz\n",
    "        print(\"domain-graph.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/Dutch/css-file-information.csv\"):\n",
    "        print(\"css-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/TextFilesInformationExtraction/css-file-information.csv.gz?access=IGBKSGXJBL3L4IJ2LXS5NPJTOY77JCT5\" --output data/Dutch/css-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"css-file-information.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/Dutch/js-file-information.csv\"):\n",
    "        print(\"js-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/TextFilesInformationExtraction/js-file-information.csv.gz?access=RPZKGWUTWJBIRKOKKHGECB7W4OHUATMO\" --output data/Dutch/js-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"js-file-information.csv already exists\")\n",
    "    \n",
    "    if not os.path.exists(\"data/Dutch/html-file-information.csv\"):\n",
    "        print(\"html-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/TextFilesInformationExtraction/html-file-information.csv.gz?access=52ZOSNQQMFWKW42WMQPNMGKYLEVUQHUC\" --output data/Dutch/html-file-information.csv.gz\n",
    "        print(\"html-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"html-file-information.csv already exists\")\n",
    "\n",
    "unzip = \"find data/\" +Dataset +\" -name '*.gz' -exec gunzip {} \\;\"\n",
    "!{unzip}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b15f4-e3f6-4f41-b692-d6971edc4147",
   "metadata": {
    "tags": []
   },
   "source": [
    "# General helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9143d",
   "metadata": {},
   "source": [
    "## Apply patch to fix pandas memory leak issue\n",
    "\n",
    "The code bellow comes from https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442  \n",
    "It solves a memory leak issue caused by pandas using a library that does not release memory correctly causing memory to fill up over time and eventually crash if RAM is exceded.\n",
    "\n",
    "This may not be a problem for smaller datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "485b627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying monkeypatch for pd.DataFrame.__del__\n"
     ]
    }
   ],
   "source": [
    "# monkeypatches.py\n",
    "\n",
    "# Solving memory leak problem in pandas\n",
    "# https://github.com/pandas-dev/pandas/issues/2659#issuecomment-12021083\n",
    "import sys\n",
    "import pandas as pd\n",
    "from ctypes import cdll, CDLL\n",
    "try:\n",
    "    cdll.LoadLibrary(\"libc.so.6\")\n",
    "    libc = CDLL(\"libc.so.6\")\n",
    "    libc.malloc_trim(0)\n",
    "except (OSError, AttributeError):\n",
    "    libc = None\n",
    "\n",
    "__old_del = getattr(pd.DataFrame, '__del__', None)\n",
    "\n",
    "def __new_del(self):\n",
    "    if __old_del:\n",
    "        __old_del(self)\n",
    "    libc.malloc_trim(0)\n",
    "\n",
    "if libc:\n",
    "    print('Applying monkeypatch for pd.DataFrame.__del__', file=sys.stderr)\n",
    "    pd.DataFrame.__del__ = __new_del\n",
    "else:\n",
    "    print('Skipping monkeypatch for pd.DataFrame.__del__: libc or malloc_trim() not found', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91961c6-b17c-4714-9f03-e1b9ea0befce",
   "metadata": {},
   "source": [
    "## Convert a full url to a domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19d1f0a-3fcf-45b7-8f58-f829a989cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import parse\n",
    "\n",
    "def to_domain(full_url):\n",
    "    netloc = parse.urlparse(full_url).netloc\n",
    "    if netloc.startswith('www.'):\n",
    "        netloc = netloc[4:]\n",
    "    return netloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee76668-8aae-420d-8194-b8175e8a3b50",
   "metadata": {},
   "source": [
    "## Filter dataframe with regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e5d7c-c88c-4103-976e-e3004eaa114e",
   "metadata": {},
   "source": [
    "The **chunksize** variable determines how much memory is used. A larger chunksize allows for faster computation, but will crash when RAM is full. If the computation crashes try restarting the kernel and/or a lower chunksize. A chunksize of 1000 should be fine for most machines. On our machine memory is not an issue, but still it is something to consider when running the same code on different machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e09ba5c4-403e-416d-9100-1774d82d4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc # For freeing up RAM\n",
    "#import pandas as pd\n",
    "\n",
    "def filter_df_content_regex(file, regex, chunksize):\n",
    "    list_regex = [] # Appending to list before conferting to dataframe because it is computationally cheaper\n",
    "    \n",
    "    # Loops through the CSV in chunks and saves those that match the regex in the content column \n",
    "    for chunk in pd.read_csv(file, iterator=True, chunksize=chunksize):\n",
    "        # Add all rows with the regex in the content column to list_regex\n",
    "        list_regex.append(chunk[chunk.content.str.contains(regex, regex= True, na=False)])\n",
    "        # chunk_offset = chunk_offset + chunksize\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    filtered_df = pd.concat(list_regex, ignore_index=True) # Convert the list to a pandas dataframe\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ffc74-d83b-489c-b854-62901645eba0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filter dataframe by date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ce736-9470-4d69-96a8-4e2b8e5ee7f9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b7c572f-3d3b-4ba3-9bf0-d5087aa6a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_date(file, begin_date, end_date , chunksize):\n",
    "    list = [] # Appending to list before conferting to dataframe because it is computationally cheaper\n",
    "    \n",
    "    # Loops through the CSV in chunks and saves those that match the given timeframe\n",
    "    for chunk in pd.read_csv(file, iterator=True, chunksize=chunksize):\n",
    "        # Add all rows that match the given time frame\n",
    "        list.append(chunk[(chunk['crawl_date'] >= begin_date) & (chunk['crawl_date'] <= end_date)])\n",
    "        # chunk_offset = chunk_offset + chunksize\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    filtered_df = pd.concat(list, ignore_index=True) # Convert the list to a pandas dataframe\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed06fa0-ec98-4c44-9036-050d1ce7e638",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Add an Internet archive Wayback Machine link to a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7eac9c",
   "metadata": {},
   "source": [
    "Use the ``crawl_date`` and ``url`` columns to estimate the Internet Archive link. The CSV only provides the date of the crawl, not the time, so the resulting link may be for the wrong capure if more than one capture is available on that date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a851b74e-6e49-41a0-9c8b-1cdcd8b40ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ia_url(df):\n",
    "    IA_url = []\n",
    "    for index, row in df.iterrows():\n",
    "        url = \"https://web.archive.org/web/\" + str(row['crawl_date']) + \"/\" + row[\"url\"]\n",
    "        IA_url.append(url)\n",
    "    df['IA_url'] = IA_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c57ffb4-bbef-4348-9df7-08b0cf1654fb",
   "metadata": {},
   "source": [
    "## Display a specific comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb5879f1-2894-45ce-bce2-c546e3e61bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comment(dataframe, index):\n",
    "    print(dataframe['comments'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4248e-230c-43fc-a18e-73f456be1247",
   "metadata": {},
   "source": [
    "## Intersection of two dataframes\n",
    "\n",
    "returns a dataframe with the intersection (overlap) of two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b61d82-9e07-4907-ba70-39883f5bc788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_intersection(df1,df2):\n",
    "    return pd.concat([df1,df2])[pd.concat([df1,df2]).duplicated(keep=False)].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e793373-6ddc-415b-8790-8b416c0ccca4",
   "metadata": {},
   "source": [
    "## Dataframe difference\n",
    "\n",
    "returns a dataframe with all entries in each dataframe but not those in both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73bcc59d-18bb-4ed8-ab0a-7d8db922dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_difference(df1,df2):\n",
    "    return pd.concat([df1,df2]).drop_duplicates(keep=False, ignore_index=True)\n",
    "\n",
    "    #pd.concat([df2, df1, df1]).drop_duplicates(keep=False) # would show what is in dataframe 2 that is not in dataframe 1 #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7a4bf-531d-40c9-bc19-2e28234c50c9",
   "metadata": {},
   "source": [
    "## Restrict domain list\n",
    "\n",
    "The code below filters out unwanted domains from the dataset an creates a new CSV file.  \n",
    "The ``domains_in_scope`` input variable is a list of the desired domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00720d75-4845-4427-988c-c7f742722384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc # For freeing up RAM\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def filter_df_domains(input_file, output_file, domains_in_scope, chunksize):\n",
    "    output = open(output_file, \"w\")\n",
    "    output.close()\n",
    "    pd.read_csv(input_file, header=None, nrows=1).to_csv(output_file, mode='a', header=False)\n",
    "    \n",
    "    # Loops through the CSV in chunks and saves those with URLs thst match those in the domains_in_scope \n",
    "    for chunk in pd.read_csv(input_file, iterator=True, chunksize=chunksize):\n",
    "        chunk=chunk[chunk.url.apply(to_domain).isin(domains_in_scope)]\n",
    "        chunk.to_csv(output_file, mode='a', header=False)\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "#domains_in_scope = [\"ksta.de\"]\n",
    "#filter_df_domains(\"data/German/html-file-information.csv\", \"data/German/ksta.csv\", domains_in_scope, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a166ff9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract wepages holding disqus.com/embed.js in their HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9766d4a",
   "metadata": {},
   "source": [
    "The code bellow looks through a dataset for any mention of disqus.js, an indicator of the presence of the disqus commenting system. The input for ```filter_df_content_regex``` is the desired dataset file, the regular expression and the chunksize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93892bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options are \"data/German/html-file-information.csv\" \"data/World/html-file-information.csv\" and \"data/Dutch/html-file-information.csv\"\n",
    "# or any other dataset on the system.\n",
    "disqus_df = filter_df_content_regex(\"data/World/html-file-information.csv\", \"(?i)disqus\\.com\\/embed\\.js\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb3b942-d0a5-4157-a1dc-49c21c9ec616",
   "metadata": {},
   "source": [
    "Count the pages with disqus.js in their HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f575b650-f771-40a7-8d8d-c753bc549595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6840"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(disqus_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef06e55",
   "metadata": {},
   "source": [
    "Note that this finds more hits than grep. It seems the csv file is too large for grep. the same query in awk does give the expected number of results\n",
    "\n",
    "```grep -i -n 'disqus\\.js' C/Users/rjans/Desktop/ARCH/Data/disqus.csv```  \n",
    "vs  \n",
    "```awk -e '/disqus\\.js/ {print $0}' C/Users/rjans/Desktop/ARCH/Data/html-file-information-German.csv```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde1c43-e9d7-47a8-bc1e-aea6a3270ac1",
   "metadata": {},
   "source": [
    "Show the unique domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0170c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aljazeera.com' 'blogs.aljazeera.com' 'men.24.com' 'blogs.abcnews.com'\n",
      " 'bloomberg.com' 'popwatch.ew.com' 'cnn.com' 'abcnews.go.com'\n",
      " 'liveshots.blogs.foxnews.com' 'magazine.foxnews.com'\n",
      " 'gretawire.foxnewsinsider.com' 'csmonitor.com' 'photos.essence.com'\n",
      " 'myfoxchicago.com' 'myfoxla.com' 'myfoxmemphis.com' 'myfoxorlando.com'\n",
      " 'mercurynews.com' 'spokesman.com' 'sfist.com' 'blogs.sfweekly.com'\n",
      " 'tpmmuckraker.talkingpointsmemo.com' 'topgear.com' 'mirror.co.uk'\n",
      " 'fashion.telegraph.co.uk' 'mweb.co.za' 'sagoodnews.co.za'\n",
      " 'independent.co.uk' 'telegraph.co.uk' 'rawstory.com' 'pcmag.com'\n",
      " 'politico.com' 'voices.news24.com' 'newser.com' 'newsmax.com'\n",
      " 'english.aljazeera.net' 'lazygamer.net' 'washingtontimes.com'\n",
      " 'communities.washingtontimes.com' 'propublica.org' 'scpr.org']\n"
     ]
    }
   ],
   "source": [
    "print(disqus_df['url'].apply(to_domain).unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd05ef",
   "metadata": {},
   "source": [
    "Save the results in a seperate CSV (only needs to be done once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccb954b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disqus_df.to_csv(\"disqus.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a04515",
   "metadata": {},
   "source": [
    "Convert **content** column to single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcaa1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in disqus_df.content:\n",
    "    disqus_df.content = disqus_df.content.replace(line, \" \".join(line.splitlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412449fe",
   "metadata": {},
   "source": [
    "Save as single line content in a seperate CSV (only needs to be done once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63ea37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disqus_df.to_csv(\"disqus_single_line.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250400c",
   "metadata": {},
   "source": [
    "# For the found pages extract the comment structure\n",
    "\n",
    "The code below requires beautifulsoup4. use the commented out import and pip command to install the library in the right place if you do not already beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b652f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00a560",
   "metadata": {},
   "source": [
    "Extract the disqus comments from the div tag with ```id=\"disqus_thread\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fafe251",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "for line in disqus_df.content:\n",
    "    soup = BeautifulSoup(line, 'html.parser')\n",
    "    try:\n",
    "        comments.append(soup.find('div', attrs={'id': 'disqus_thread'}).prettify()) # .prettify() makes the HTML of the comments more human readable\n",
    "    except AttributeError:\n",
    "        comments.append(\"\")\n",
    "        \n",
    "disqus_df['comments'] = comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da28729a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>url</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20140502</td>\n",
       "      <td>http://www.aljazeera.com/news/middleeast/2014/...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20140501</td>\n",
       "      <td>http://www.aljazeera.com/news/middleeast/2014/...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20140502</td>\n",
       "      <td>http://www.aljazeera.com/news/middleeast/2014/...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20140531</td>\n",
       "      <td>http://www.aljazeera.com/news/middleeast/2014/...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20150102</td>\n",
       "      <td>http://www.aljazeera.com/news/middleeast/2014/...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>20120105</td>\n",
       "      <td>http://www.independent.co.uk/environment/natur...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>20110301</td>\n",
       "      <td>http://www.independent.co.uk/environment/scien...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div class=\"clearfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6837</th>\n",
       "      <td>20120105</td>\n",
       "      <td>http://www.independent.co.uk/environment/the-1...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6838</th>\n",
       "      <td>20130502</td>\n",
       "      <td>http://www.independent.co.uk/extras/indybest/o...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6839</th>\n",
       "      <td>20120120</td>\n",
       "      <td>http://www.independent.co.uk/extras/puzzles-an...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6840 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      crawl_date                                                url  \\\n",
       "0       20140502  http://www.aljazeera.com/news/middleeast/2014/...   \n",
       "1       20140501  http://www.aljazeera.com/news/middleeast/2014/...   \n",
       "2       20140502  http://www.aljazeera.com/news/middleeast/2014/...   \n",
       "3       20140531  http://www.aljazeera.com/news/middleeast/2014/...   \n",
       "4       20150102  http://www.aljazeera.com/news/middleeast/2014/...   \n",
       "...          ...                                                ...   \n",
       "6835    20120105  http://www.independent.co.uk/environment/natur...   \n",
       "6836    20110301  http://www.independent.co.uk/environment/scien...   \n",
       "6837    20120105  http://www.independent.co.uk/environment/the-1...   \n",
       "6838    20130502  http://www.independent.co.uk/extras/indybest/o...   \n",
       "6839    20120120  http://www.independent.co.uk/extras/puzzles-an...   \n",
       "\n",
       "                                               comments  \n",
       "0                    <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "1                    <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "2                    <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "3                    <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "4                    <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "...                                                 ...  \n",
       "6835                 <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "6836  <div id=\"disqus_thread\">\\n <div class=\"clearfi...  \n",
       "6837                 <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "6838                 <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "6839                 <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "\n",
       "[6840 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_colwidth', 500) \n",
    "disqus_df[['crawl_date','url','comments']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea198f9-e4da-42e0-b35f-55d73ca7065e",
   "metadata": {},
   "source": [
    "# Add an internet archive URL to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb52f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_ia_url(disqus_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948144a",
   "metadata": {},
   "source": [
    "Filter out comment fields that are so short that they do net contain comments (based on the length).  \n",
    "32 characters is currently the shortest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dcaebce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>IA_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>&lt;div class=\"loading\" id=\"disqus_thread\"&gt;\\n&lt;/di...</td>\n",
       "      <td>https://web.archive.org/web/20150522/http://bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>&lt;div class=\"loading\" id=\"disqus_thread\"&gt;\\n&lt;/di...</td>\n",
       "      <td>https://web.archive.org/web/20150511/http://bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;script&gt;\\n  var dis...</td>\n",
       "      <td>https://web.archive.org/web/20150127/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;script&gt;\\n  var dis...</td>\n",
       "      <td>https://web.archive.org/web/20140301/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>&lt;div class=\"loading\" id=\"disqus_thread\"&gt;\\n&lt;/di...</td>\n",
       "      <td>https://web.archive.org/web/20150505/http://ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5529</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div class=\"clearfi...</td>\n",
       "      <td>https://web.archive.org/web/20110412/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>&lt;div class=\"\" id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "      <td>https://web.archive.org/web/20130304/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6825</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div class=\"clearfi...</td>\n",
       "      <td>https://web.archive.org/web/20110408/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6828</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div class=\"clearfi...</td>\n",
       "      <td>https://web.archive.org/web/20110927/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div class=\"clearfi...</td>\n",
       "      <td>https://web.archive.org/web/20110301/http://ww...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comments  \\\n",
       "235   <div class=\"loading\" id=\"disqus_thread\">\\n</di...   \n",
       "240   <div class=\"loading\" id=\"disqus_thread\">\\n</di...   \n",
       "248   <div id=\"disqus_thread\">\\n <script>\\n  var dis...   \n",
       "249   <div id=\"disqus_thread\">\\n <script>\\n  var dis...   \n",
       "734   <div class=\"loading\" id=\"disqus_thread\">\\n</di...   \n",
       "...                                                 ...   \n",
       "5529  <div id=\"disqus_thread\">\\n <div class=\"clearfi...   \n",
       "5570        <div class=\"\" id=\"disqus_thread\">\\n</div>\\n   \n",
       "6825  <div id=\"disqus_thread\">\\n <div class=\"clearfi...   \n",
       "6828  <div id=\"disqus_thread\">\\n <div class=\"clearfi...   \n",
       "6836  <div id=\"disqus_thread\">\\n <div class=\"clearfi...   \n",
       "\n",
       "                                                 IA_url  \n",
       "235   https://web.archive.org/web/20150522/http://bl...  \n",
       "240   https://web.archive.org/web/20150511/http://bl...  \n",
       "248   https://web.archive.org/web/20150127/http://ww...  \n",
       "249   https://web.archive.org/web/20140301/http://ww...  \n",
       "734   https://web.archive.org/web/20150505/http://ab...  \n",
       "...                                                 ...  \n",
       "5529  https://web.archive.org/web/20110412/http://ww...  \n",
       "5570  https://web.archive.org/web/20130304/http://ww...  \n",
       "6825  https://web.archive.org/web/20110408/http://ww...  \n",
       "6828  https://web.archive.org/web/20110927/http://ww...  \n",
       "6836  https://web.archive.org/web/20110301/http://ww...  \n",
       "\n",
       "[572 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_colwidth', 50)\n",
    "disqus_df[disqus_df['comments'].apply(lambda x: len(x)>32)][['comments','IA_url']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8bbf3-2aab-4834-b47b-b6dc68c96091",
   "metadata": {},
   "source": [
    "Display a specific comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3561edde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div id=\"disqus_thread\">\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_comment(disqus_df, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6937ee4c-accc-4884-9d85-f16949253615",
   "metadata": {},
   "source": [
    "# Comparing dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b294e8d2-adc3-45ec-8635-0663b070e8d5",
   "metadata": {},
   "source": [
    "Creates a second dataframe with a different regex query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f46fa4ce-138a-4b89-8cea-b2562323d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "disqus_df_2 = filter_df_content_regex(\"data/World/html-file-information.csv\", \"(?i)disqus\\.js\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726b6811-cda7-46a5-a874-bb76fbee9cfa",
   "metadata": {},
   "source": [
    "Show the overlap of two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2f07cdf-497c-4bff-8b3b-11d9cb2fb5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>224</td>\n",
       "      <td>20180104</td>\n",
       "      <td>https://men.24.com/watch-five-crazy-sport-fans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>225</td>\n",
       "      <td>20180104</td>\n",
       "      <td>https://men.24.com/watch-five-crazy-sport-fans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>680</td>\n",
       "      <td>20140401</td>\n",
       "      <td>http://popwatch.ew.com/2014/03/31/how-i-met-yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>681</td>\n",
       "      <td>20140401</td>\n",
       "      <td>http://popwatch.ew.com/2014/03/31/how-i-met-yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>706</td>\n",
       "      <td>20100318</td>\n",
       "      <td>http://www.cnn.com/2009/SHOWBIZ/Music/12/31/ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1527</td>\n",
       "      <td>20170301</td>\n",
       "      <td>http://www.washingtontimes.com/news/2017/feb/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1530</td>\n",
       "      <td>20170301</td>\n",
       "      <td>http://www.washingtontimes.com/news/2017/feb/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1531</td>\n",
       "      <td>20170301</td>\n",
       "      <td>http://www.washingtontimes.com/news/2017/feb/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1913</td>\n",
       "      <td>20180201</td>\n",
       "      <td>https://www.washingtontimes.com/news/2018/jan/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>1914</td>\n",
       "      <td>20180201</td>\n",
       "      <td>https://www.washingtontimes.com/news/2018/jan/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  crawl_date                                                url\n",
       "0      224    20180104  https://men.24.com/watch-five-crazy-sport-fans...\n",
       "1      225    20180104  https://men.24.com/watch-five-crazy-sport-fans...\n",
       "2      680    20140401  http://popwatch.ew.com/2014/03/31/how-i-met-yo...\n",
       "3      681    20140401  http://popwatch.ew.com/2014/03/31/how-i-met-yo...\n",
       "4      706    20100318  http://www.cnn.com/2009/SHOWBIZ/Music/12/31/ch...\n",
       "..     ...         ...                                                ...\n",
       "207   1527    20170301  http://www.washingtontimes.com/news/2017/feb/2...\n",
       "208   1530    20170301  http://www.washingtontimes.com/news/2017/feb/2...\n",
       "209   1531    20170301  http://www.washingtontimes.com/news/2017/feb/2...\n",
       "210   1913    20180201  https://www.washingtontimes.com/news/2018/jan/...\n",
       "211   1914    20180201  https://www.washingtontimes.com/news/2018/jan/...\n",
       "\n",
       "[212 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_intersection(disqus_df[['crawl_date','url']],disqus_df_2[['crawl_date','url']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3881b85-838c-4868-bf10-3fa6ffe5efc6",
   "metadata": {},
   "source": [
    "Show the difference between the two datasets. So only shows results that are in one dataset but not in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c918052c-6f87-4029-93ec-9cac545d3d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20140502</td>\n",
       "      <td>http://www.aljazeera.com/news/middleeast/2014/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20140501</td>\n",
       "      <td>http://www.aljazeera.com/news/middleeast/2014/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20140502</td>\n",
       "      <td>http://www.aljazeera.com/news/middleeast/2014/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20140531</td>\n",
       "      <td>http://www.aljazeera.com/news/middleeast/2014/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20150102</td>\n",
       "      <td>http://www.aljazeera.com/news/middleeast/2014/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8539</th>\n",
       "      <td>20180201</td>\n",
       "      <td>https://www.washingtontimes.com/news/2018/jan/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8540</th>\n",
       "      <td>20180201</td>\n",
       "      <td>https://www.washingtontimes.com/news/2018/jan/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8541</th>\n",
       "      <td>20180201</td>\n",
       "      <td>https://www.washingtontimes.com/news/2018/jan/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8542</th>\n",
       "      <td>20180201</td>\n",
       "      <td>https://www.washingtontimes.com/news/2018/jan/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8543</th>\n",
       "      <td>20180201</td>\n",
       "      <td>https://www.washingtontimes.com/news/2018/jan/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8544 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      crawl_date                                                url\n",
       "0       20140502  http://www.aljazeera.com/news/middleeast/2014/...\n",
       "1       20140501  http://www.aljazeera.com/news/middleeast/2014/...\n",
       "2       20140502  http://www.aljazeera.com/news/middleeast/2014/...\n",
       "3       20140531  http://www.aljazeera.com/news/middleeast/2014/...\n",
       "4       20150102  http://www.aljazeera.com/news/middleeast/2014/...\n",
       "...          ...                                                ...\n",
       "8539    20180201  https://www.washingtontimes.com/news/2018/jan/...\n",
       "8540    20180201  https://www.washingtontimes.com/news/2018/jan/...\n",
       "8541    20180201  https://www.washingtontimes.com/news/2018/jan/...\n",
       "8542    20180201  https://www.washingtontimes.com/news/2018/jan/...\n",
       "8543    20180201  https://www.washingtontimes.com/news/2018/jan/...\n",
       "\n",
       "[8544 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_difference(disqus_df[['crawl_date','url']],disqus_df_2[['crawl_date','url']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c5db7-1ae4-487c-b16a-d647cbd1937e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
