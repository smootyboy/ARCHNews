{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348e893b-e773-4e96-956d-74e6fd110ef4",
   "metadata": {},
   "source": [
    "# Download an archive\n",
    "The script below makes a data folder with a subfolder for the dataset. Dataset option are \"World\", \"German\" or \"Dutch\", select these by altering the dataset variable. The script wil skip files that are already present on the system. If you want to redownload the files first remove the old files manually.  \n",
    "If the download fails due to connection errors, rerun the script and will will notice that he file did not decompress and try again.  \n",
    "Note: The html-file-information.csv can be very large and take several hours and fail. Consider putting the code below code in a loop if you want to run it over night, if the download fails it will try again and if successfull it will say it already has the desired file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77006d6e-6e30-43b3-9353-37de21a7e004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "mkdir: cannot create directory ‘data/World’: File exists\n",
      "domain-frequency.csv already exists\n",
      "domain-frequency.csv already exists\n",
      "css-file-information.csv already exists\n",
      "js-file-information.csv already exists\n",
      "html-file-information.csv is not present\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 6594M    0 6594M    0     0   318k      0 --:--:--  5:53:49 --:--:--  203k-:--  0:06:57 --:--:--  352k 0:11:49 --:--:--  200k:--:--  511k6k519k-:--:--  0:27:20 --:--:--  269k  343k-:--  0:40:46 --:--:--  410k 0:43:06 --:--:--  283k0M    0     0   380k      0 --:--:--  0:43:34 --:--:--  368k-:--:--  283k54:19 --:--:--  368k  1:02:14 --:--:-- 1212k--  377k--:--  1:07:29 --:--:--  646k--:--  1:09:09 --:--:--  301k--  1:11:51 --:--:--  326k  321k:12:13 --:--:--  361k   0 --:--:--  1:28:32 --:--:--  715k--:--:--  1:30:33 --:--:--  327k:--  582k    0 --:--:--  1:42:00 --:--:--  289k:--:--  669k  322k--  2:22:59 --:--:--  227k      0 --:--:--  2:29:27 --:--:--  381k242k--:--  2:34:08 --:--:--  287k:--  2:34:44 --:--:--  405k-:--  238k8k-:--:--  182k 3:06:46 --:--:--  310k  0     0   350k      0 --:--:--  3:18:19 --:--:--  250k 3:26:10 --:--:--  249k:--  3:32:17 --:--:--  377k:34:20 --:--:--  242k --:--:--  3:36:17 --:--:--  272k  3:50:45 --:--:--  213k:--:--  237k-:--:--  4:05:01 --:--:--  172k--:--  144k   330k      0 --:--:--  4:28:55 --:--:--  248k332k  0 --:--:--  4:35:07 --:--:--  275kk--  218k-:--  217k 301k:--:--  285k4k6k   0 --:--:--  5:19:06 --:--:--  218k--:--  171k:--:--  271k\n",
      "html-file-information.csv has been downloaded\n"
     ]
    }
   ],
   "source": [
    "# Dataset option are \"World\", \"German\" or \"Dutch\"\n",
    "Dataset = \"World\"\n",
    "\n",
    "!mkdir data\n",
    "!mkdir data/{Dataset}\n",
    "\n",
    "import os.path\n",
    "\n",
    "if Dataset == \"German\":\n",
    "    if not os.path.exists(\"data/German/domain-frequency.csv\"):\n",
    "        print(\"domain-frequency.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/DomainFrequencyExtraction/domain-frequency.csv.gz?access=VRQ4COI5RFEB6XTJZNQTBRLEZTTHJERL\" --output data/German/domain-frequency.csv.gz\n",
    "        print(\"domain-frequency.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/German/domain-graph.csv\"):\n",
    "        print(\"domain-graph.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/DomainGraphExtraction/domain-graph.csv.gz?access=JKEPGQ6MUC72JQB23IXOC4KOLGJYDSMN\" --output data/German/domain-graph.csv.gz\n",
    "        print(\"domain-graph.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/German/css-file-information.csv\"):\n",
    "        print(\"css-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/css-file-information.csv.gz?access=I2WP4REJA3NOBU3TCAAL3OIGJKNXM46R\" --output data/German/css-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"css-file-information.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/German/js-file-information.csv\"):\n",
    "        print(\"js-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/js-file-information.csv.gz?access=M3QSMFPLEHPZPWZIFSMZ6CT2OO7WYQ4M\" --output data/German/js-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"js-file-information.csv already exists\")\n",
    "        \n",
    "    if not os.path.exists(\"data/German/html-file-information.csv\"):\n",
    "        print(\"html-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/html-file-information.csv.gz?access=E3BUHKL5P4Q3TD4LGXZV2AOMERYQ3GWL\" --output data/German/html-file-information.csv.gz\n",
    "        print(\"html-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"html-file-information.csv already exists\")\n",
    "        \n",
    "if Dataset == \"World\":\n",
    "    if not os.path.exists(\"data/World/domain-frequency.csv\"):\n",
    "        print(\"domain-frequency.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/DomainFrequencyExtraction/domain-frequency.csv.gz?access=SMSQY3G6IGKGRWVLGCWMA7DMCHBCKQ4K\" --output data/World/domain-frequency.csv.gz\n",
    "        print(\"domain-frequency.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/World/domain-graph.csv\"):\n",
    "        print(\"domain-graph.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/DomainGraphExtraction/domain-graph.csv.gz?sample=true&access=BZTA7LW5LUNMPGKWQMLKAUAXBV2E2AEC\" --output data/World/domain-graph.csv.gz\n",
    "        print(\"domain-graph.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/World/css-file-information.csv\"):\n",
    "        print(\"css-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/css-file-information.csv.gz?access=UZ4PWVRXXWPF53BHM7TTYNK24P7YAIXQ\" --output data/World/css-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"css-file-information.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/World/js-file-information.csv\"):\n",
    "        print(\"js-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/js-file-information.csv.gz?access=54X7IV7QOOAJWGHKPRGDI7HIR6W6GKPI\" --output data/World/js-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"js-file-information.csv already exists\")\n",
    "    \n",
    "    if not os.path.exists(\"data/World/html-file-information.csv\"):\n",
    "        print(\"html-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/html-file-information.csv.gz?access=JQEDT3PRXOS6OXZ7LA5EC55HQJVUASDX\" --output data/World/html-file-information.csv.gz\n",
    "        print(\"html-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"html-file-information.csv already exists\")\n",
    "\n",
    "if Dataset == \"Dutch\":\n",
    "    if not os.path.exists(\"data/Dutch/domain-frequency.csv\"):\n",
    "        print(\"domain-frequency.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/DomainFrequencyExtraction/domain-frequency.csv.gz?access=QUQTRHVNIDKXN62S4D62XFHCFI7VDSH7\" --output data/Dutch/domain-frequency.csv.gz\n",
    "        print(\"domain-frequency.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/Dutch/domain-graph.csv\"):\n",
    "        print(\"domain-graph.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/DomainGraphExtraction/domain-graph.csv.gz?access=JUNESOMDTBYDNCCGVJI5MJHW45KHAIJZ\" --output data/Dutch/domain-graph.csv.gz\n",
    "        print(\"domain-graph.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/Dutch/css-file-information.csv\"):\n",
    "        print(\"css-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/TextFilesInformationExtraction/css-file-information.csv.gz?access=IGBKSGXJBL3L4IJ2LXS5NPJTOY77JCT5\" --output data/Dutch/css-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"css-file-information.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/Dutch/js-file-information.csv\"):\n",
    "        print(\"js-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/TextFilesInformationExtraction/js-file-information.csv.gz?access=RPZKGWUTWJBIRKOKKHGECB7W4OHUATMO\" --output data/Dutch/js-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"js-file-information.csv already exists\")\n",
    "    \n",
    "    if not os.path.exists(\"data/Dutch/html-file-information.csv\"):\n",
    "        print(\"html-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/TextFilesInformationExtraction/html-file-information.csv.gz?access=52ZOSNQQMFWKW42WMQPNMGKYLEVUQHUC\" --output data/Dutch/html-file-information.csv.gz\n",
    "        print(\"html-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"html-file-information.csv already exists\")\n",
    "\n",
    "!find data/{Dataset} -name '*.gz' -exec gunzip {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9143d",
   "metadata": {},
   "source": [
    "# Apply patch to fix pandas memory leak issue\n",
    "\n",
    "The code bellow comes from https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442  \n",
    "It solves a memory leak issue caused by pandas using a library that does not release memory correctly causing memory to fill up over time and eventually crash if RAM is exceded.\n",
    "\n",
    "This may not be a problem for smaller datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "485b627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying monkeypatch for pd.DataFrame.__del__\n"
     ]
    }
   ],
   "source": [
    "# monkeypatches.py\n",
    "\n",
    "# Solving memory leak problem in pandas\n",
    "# https://github.com/pandas-dev/pandas/issues/2659#issuecomment-12021083\n",
    "import sys\n",
    "import pandas as pd\n",
    "from ctypes import cdll, CDLL\n",
    "try:\n",
    "    cdll.LoadLibrary(\"libc.so.6\")\n",
    "    libc = CDLL(\"libc.so.6\")\n",
    "    libc.malloc_trim(0)\n",
    "except (OSError, AttributeError):\n",
    "    libc = None\n",
    "\n",
    "__old_del = getattr(pd.DataFrame, '__del__', None)\n",
    "\n",
    "def __new_del(self):\n",
    "    if __old_del:\n",
    "        __old_del(self)\n",
    "    libc.malloc_trim(0)\n",
    "\n",
    "if libc:\n",
    "    print('Applying monkeypatch for pd.DataFrame.__del__', file=sys.stderr)\n",
    "    pd.DataFrame.__del__ = __new_del\n",
    "else:\n",
    "    print('Skipping monkeypatch for pd.DataFrame.__del__: libc or malloc_trim() not found', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7a4bf-531d-40c9-bc19-2e28234c50c9",
   "metadata": {},
   "source": [
    "# Restrict domain list\n",
    "\n",
    "The code below filters out unwanted domains from the dataset an creates a new CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fb856-cfc3-49b5-9d25-8376ce3e78ef",
   "metadata": {},
   "source": [
    "The desired domains are extracted from a column labled \"URL\" in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f6e5fc7-5cd9-40e1-a8de-7ab0102dc516",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list = pd.read_csv(\"Data/German/Top News German news Websites.csv\", usecols=[\"URL\"])\n",
    "\n",
    "from urllib import parse\n",
    "\n",
    "def to_domain(full_url):\n",
    "    netloc = parse.urlparse(full_url).netloc\n",
    "    if netloc.startswith('www.'):\n",
    "        netloc = netloc[4:]\n",
    "    return netloc\n",
    "\n",
    "domains_in_scope = domains_list[\"URL\"].apply(to_domain) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e5d7c-c88c-4103-976e-e3004eaa114e",
   "metadata": {},
   "source": [
    "The **chunksize** variable determines how much memory is used. A larger chunksize allows for faster computation, but will crash when RAM is full. If the computation crashes try restarting the kernel and/or a lower chunksize. A chunksize of 1000 should be fine for most machines. On our machine memory is not an issue, but still it is something to consider when running the same code on different machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00720d75-4845-4427-988c-c7f742722384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc # For freeing up RAM\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def filter_df_domains(input_file, output_file, chunksize):\n",
    "    output = open(output_file, \"w\")\n",
    "    output.close()\n",
    "    pd.read_csv(input_file, header=None, nrows=1).to_csv(output_file, mode='a', header=False)\n",
    "    \n",
    "    # Loops through the CSV in chunks and saves those with URLs thst match those in the domains_in_scope \n",
    "    for chunk in pd.read_csv(input_file, iterator=True, chunksize=chunksize):\n",
    "        chunk=chunk[chunk.url.apply(to_domain).isin(domains_in_scope)]\n",
    "        chunk.to_csv(output_file, mode='a', header=False)\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "filter_df_domains(\"Data/German/html-file-information.csv\", \"Data/German/inScope.csv\", 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a166ff9",
   "metadata": {},
   "source": [
    "# Extract wepages holding disqus.js in their HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9766d4a",
   "metadata": {},
   "source": [
    "The code bellow Looks through a dataset for any mention of disqus.js, an indicator of the presence of the disqus commenting system. The input for ```filter_df_content``` is the desired dataset file and the chunksize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93892bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc # For freeing up RAM\n",
    "import pandas as pd\n",
    "\n",
    "def filter_df_content_regex(file, regex, chunksize):\n",
    "    list_disqus = [] # Appending to list before conferting to dataframe because it is computationally cheaper\n",
    "    \n",
    "    # Loops through the CSV in chunks and saves those with disqus.js in the content column \n",
    "    for chunk in pd.read_csv(file, iterator=True, chunksize=chunksize):\n",
    "        # Add all rows with \"disqus.js\" in the content column to list_disqus\n",
    "        list_disqus.append(chunk[chunk.content.str.contains(regex, regex= True, na=False)])\n",
    "        # chunk_offset = chunk_offset + chunksize\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    disqus_df = pd.concat(list_disqus, ignore_index=True) # Convert the list to a pandas dataframe\n",
    "\n",
    "    return disqus_df\n",
    "\n",
    "# Options are \"Data/German/html-file-information.csv\" \"Data/World/html-file-information.csv\" and \"Data/Dutch/html-file-information.csv\"\n",
    "# or any other dataset on the system.\n",
    "disqus_df = filter_df_content_regex(\"Data/World/html-file-information.csv\", \"(?i)disqus\\.js\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb3b942-d0a5-4157-a1dc-49c21c9ec616",
   "metadata": {},
   "source": [
    "Count the pages with disqus.js in their HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f575b650-f771-40a7-8d8d-c753bc549595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1916"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(disqus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e3cd7-e7c0-4379-acc5-c593f2c9ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Show the unique domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0170c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blogs.aljazeera.com', 'stream.aljazeera.com', 'bandwidthblog.com',\n",
       "       'globalnews.ca', 'liveshots.blogs.foxnews.com',\n",
       "       'politics.blogs.foxnews.com', 'weather.blogs.foxnews.com',\n",
       "       'foxnews.com', 'thehill.com', 'washingtonexaminer.com',\n",
       "       'blogs.independent.co.uk', 'politico.com', 'rawstory.com',\n",
       "       'sfexaminer.com', 'blogs.aljazeera.net', 'lazygamer.net',\n",
       "       'alternet.org', 'washingtontimes.com', 'wonkette.com'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disqus_df['url'].apply(to_domain).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef06e55",
   "metadata": {},
   "source": [
    "Note that this finds more hits than grep. It seems the csv file is too large for grep. the same query in awk does give the expected number of results\n",
    "\n",
    "```grep -i -n 'disqus\\.js' C/Users/rjans/Desktop/ARCH/Data/disqus.csv```  \n",
    "vs  \n",
    "```awk -e '/disqus\\.js/ {print $0}' C/Users/rjans/Desktop/ARCH/Data/html-file-information-German.csv```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd05ef",
   "metadata": {},
   "source": [
    "Save the results in a seperate CSV (only needs to be done once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccb954b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disqus_df.to_csv(\"disqus.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a04515",
   "metadata": {},
   "source": [
    "Convert **content** column to single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcaa1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in disqus_df.content:\n",
    "    disqus_df.content = disqus_df.content.replace(line, \" \".join(line.splitlines()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412449fe",
   "metadata": {},
   "source": [
    "Save as single line content in a seperate CSV (only needs to be done once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ea37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disqus_df.to_csv(\"disqus_single_line.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250400c",
   "metadata": {},
   "source": [
    "# For the found pages extract the comment structure\n",
    "\n",
    "The code below requires beautifulsoup4. use the commented out import and pip command to install the library in the right place if you do not already beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b652f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00a560",
   "metadata": {},
   "source": [
    "Extract the disqus comments from the div tag with ```id=\"disqus_thread\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fafe251",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "for line in disqus_df.content:\n",
    "    soup = BeautifulSoup(line, 'html.parser')\n",
    "    try:\n",
    "        comments.append(soup.find('div', attrs={'id': 'disqus_thread'}).prettify()) # Makes the comments more human readable\n",
    "    except AttributeError:\n",
    "        comments.append(\"\")\n",
    "        \n",
    "disqus_df['comments'] = comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da28729a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>url</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20120501</td>\n",
       "      <td>http://blogs.aljazeera.com/africa/2012/04/28/c...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20120517</td>\n",
       "      <td>http://blogs.aljazeera.com/americas/2012/03/29...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20120512</td>\n",
       "      <td>http://blogs.aljazeera.com/americas/2012/03/30...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20120501</td>\n",
       "      <td>http://blogs.aljazeera.com/americas/2012/04/28...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20120507</td>\n",
       "      <td>http://blogs.aljazeera.com/asia/2012/02/29/ser...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>20180201</td>\n",
       "      <td>https://www.washingtontimes.com/news/2018/jan/...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>20180201</td>\n",
       "      <td>https://www.washingtontimes.com/news/2018/jan/...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>20180201</td>\n",
       "      <td>https://www.washingtontimes.com/news/2018/jan/...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>20180201</td>\n",
       "      <td>https://www.washingtontimes.com/news/2018/jan/...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>20180201</td>\n",
       "      <td>https://www.washingtontimes.com/news/2018/jan/...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1916 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      crawl_date                                                url  \\\n",
       "0       20120501  http://blogs.aljazeera.com/africa/2012/04/28/c...   \n",
       "1       20120517  http://blogs.aljazeera.com/americas/2012/03/29...   \n",
       "2       20120512  http://blogs.aljazeera.com/americas/2012/03/30...   \n",
       "3       20120501  http://blogs.aljazeera.com/americas/2012/04/28...   \n",
       "4       20120507  http://blogs.aljazeera.com/asia/2012/02/29/ser...   \n",
       "...          ...                                                ...   \n",
       "1911    20180201  https://www.washingtontimes.com/news/2018/jan/...   \n",
       "1912    20180201  https://www.washingtontimes.com/news/2018/jan/...   \n",
       "1913    20180201  https://www.washingtontimes.com/news/2018/jan/...   \n",
       "1914    20180201  https://www.washingtontimes.com/news/2018/jan/...   \n",
       "1915    20180201  https://www.washingtontimes.com/news/2018/jan/...   \n",
       "\n",
       "                                comments  \n",
       "0     <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "1     <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "2     <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "3     <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "4     <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "...                                  ...  \n",
       "1911  <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "1912  <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "1913  <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "1914  <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "1915  <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "\n",
       "[1916 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_colwidth', 500) \n",
    "disqus_df[['crawl_date','url','comments']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea198f9-e4da-42e0-b35f-55d73ca7065e",
   "metadata": {},
   "source": [
    "# Add an internet archive URL to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7eac9c",
   "metadata": {},
   "source": [
    "Use the ``crawl_date`` and ``url`` columns to estimate the Internet Archive link. The CSV only provides the date of the crawl, not the time, so the resulting link may be for the wrong capure if more than one capture is available on that date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb52f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ia_url(df):\n",
    "    IA_url = []\n",
    "    for index, row in df.iterrows():\n",
    "        url = \"https://web.archive.org/web/\" + str(row['crawl_date']) + \"/\" + row[\"url\"]\n",
    "        IA_url.append(url)\n",
    "    disqus_df['IA_url'] = IA_url\n",
    "\n",
    "add_ia_url(disqus_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948144a",
   "metadata": {},
   "source": [
    "Filter out comment fields that are so short that they do net contain comments (based on the length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dcaebce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>IA_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160828/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20131208/http://gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;!-- &lt;a name=\"disqu...</td>\n",
       "      <td>https://web.archive.org/web/20120124/http://li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;!-- &lt;a name=\"disqu...</td>\n",
       "      <td>https://web.archive.org/web/20140901/http://li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;!-- &lt;a name=\"disqu...</td>\n",
       "      <td>https://web.archive.org/web/20110317/http://li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;!-- &lt;a name=\"disqu...</td>\n",
       "      <td>https://web.archive.org/web/20120417/http://li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;noscript&gt;\\n  &lt;p&gt;\\n...</td>\n",
       "      <td>https://web.archive.org/web/20160201/http://th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;noscript&gt;\\n  &lt;p&gt;\\n...</td>\n",
       "      <td>https://web.archive.org/web/20180816/http://th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;noscript&gt;\\n  &lt;p&gt;\\n...</td>\n",
       "      <td>https://web.archive.org/web/20150101/http://th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;noscript&gt;\\n  &lt;p&gt;\\n...</td>\n",
       "      <td>https://web.archive.org/web/20170303/http://th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20120104/http://bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20120501/http://bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20120501/http://bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20100102/http://ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20100102/http://ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20100102/http://ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20100102/http://ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20100102/http://ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20100102/http://ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20100102/http://ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20100102/http://ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20100102/http://ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20100102/http://ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20100102/http://ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20100102/http://ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20100102/http://ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20100102/http://ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160817/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160804/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160525/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160525/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160508/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160530/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160914/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160312/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160318/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160627/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20161020/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160525/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160428/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160821/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160324/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160229/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160617/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20160403/http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20150428/http://wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comments  \\\n",
       "18   <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "19   <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "20   <div id=\"disqus_thread\">\\n <!-- <a name=\"disqu...   \n",
       "21   <div id=\"disqus_thread\">\\n <!-- <a name=\"disqu...   \n",
       "22   <div id=\"disqus_thread\">\\n <!-- <a name=\"disqu...   \n",
       "24   <div id=\"disqus_thread\">\\n <!-- <a name=\"disqu...   \n",
       "284  <div id=\"disqus_thread\">\\n <noscript>\\n  <p>\\n...   \n",
       "285  <div id=\"disqus_thread\">\\n <noscript>\\n  <p>\\n...   \n",
       "286  <div id=\"disqus_thread\">\\n <noscript>\\n  <p>\\n...   \n",
       "287  <div id=\"disqus_thread\">\\n <noscript>\\n  <p>\\n...   \n",
       "289  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "290  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "291  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "294  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "295  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "296  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "297  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "298  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "299  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "300  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "301  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "302  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "303  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "304  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "305  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "306  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "307  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "320  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "321  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "322  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "323  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "325  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "326  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "327  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "328  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "329  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "330  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "331  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "332  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "333  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "334  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "335  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "337  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "341  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "342  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "388  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "\n",
       "                                                IA_url  \n",
       "18   https://web.archive.org/web/20160828/http://ww...  \n",
       "19   https://web.archive.org/web/20131208/http://gl...  \n",
       "20   https://web.archive.org/web/20120124/http://li...  \n",
       "21   https://web.archive.org/web/20140901/http://li...  \n",
       "22   https://web.archive.org/web/20110317/http://li...  \n",
       "24   https://web.archive.org/web/20120417/http://li...  \n",
       "284  https://web.archive.org/web/20160201/http://th...  \n",
       "285  https://web.archive.org/web/20180816/http://th...  \n",
       "286  https://web.archive.org/web/20150101/http://th...  \n",
       "287  https://web.archive.org/web/20170303/http://th...  \n",
       "289  https://web.archive.org/web/20120104/http://bl...  \n",
       "290  https://web.archive.org/web/20120501/http://bl...  \n",
       "291  https://web.archive.org/web/20120501/http://bl...  \n",
       "294  https://web.archive.org/web/20100102/http://ra...  \n",
       "295  https://web.archive.org/web/20100102/http://ra...  \n",
       "296  https://web.archive.org/web/20100102/http://ra...  \n",
       "297  https://web.archive.org/web/20100102/http://ra...  \n",
       "298  https://web.archive.org/web/20100102/http://ra...  \n",
       "299  https://web.archive.org/web/20100102/http://ra...  \n",
       "300  https://web.archive.org/web/20100102/http://ra...  \n",
       "301  https://web.archive.org/web/20100102/http://ra...  \n",
       "302  https://web.archive.org/web/20100102/http://ra...  \n",
       "303  https://web.archive.org/web/20100102/http://ra...  \n",
       "304  https://web.archive.org/web/20100102/http://ra...  \n",
       "305  https://web.archive.org/web/20100102/http://ra...  \n",
       "306  https://web.archive.org/web/20100102/http://ra...  \n",
       "307  https://web.archive.org/web/20100102/http://ra...  \n",
       "320  https://web.archive.org/web/20160817/http://ww...  \n",
       "321  https://web.archive.org/web/20160804/http://ww...  \n",
       "322  https://web.archive.org/web/20160525/http://ww...  \n",
       "323  https://web.archive.org/web/20160525/http://ww...  \n",
       "325  https://web.archive.org/web/20160508/http://ww...  \n",
       "326  https://web.archive.org/web/20160530/http://ww...  \n",
       "327  https://web.archive.org/web/20160914/http://ww...  \n",
       "328  https://web.archive.org/web/20160312/http://ww...  \n",
       "329  https://web.archive.org/web/20160318/http://ww...  \n",
       "330  https://web.archive.org/web/20160627/http://ww...  \n",
       "331  https://web.archive.org/web/20161020/http://ww...  \n",
       "332  https://web.archive.org/web/20160525/http://ww...  \n",
       "333  https://web.archive.org/web/20160428/http://ww...  \n",
       "334  https://web.archive.org/web/20160821/http://ww...  \n",
       "335  https://web.archive.org/web/20160324/http://ww...  \n",
       "337  https://web.archive.org/web/20160229/http://ww...  \n",
       "341  https://web.archive.org/web/20160617/http://ww...  \n",
       "342  https://web.archive.org/web/20160403/http://ww...  \n",
       "388  https://web.archive.org/web/20150428/http://wo...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_colwidth', 50)\n",
    "disqus_df[disqus_df['comments'].apply(lambda x: len(x)>32)][['comments','IA_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3561edde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div id=\"disqus_thread\">\\n <div id=\"dsq-content\">\\n  <ul id=\"dsq-comments\">\\n   <li class=\"comment even thread-even depth-1\" id=\"dsq-comment-384\">\\n    <div class=\"dsq-comment-header\" id=\"dsq-comment-header-384\">\\n     <cite id=\"dsq-cite-384\">\\n      <span id=\"dsq-author-user-384\">\\n       Bolko Schröder\\n      </span>\\n     </cite>\\n    </div>\\n    <div class=\"dsq-comment-body\" id=\"dsq-comment-body-384\">\\n     <div class=\"dsq-comment-message\" id=\"dsq-comment-message-384\">\\n      <p>\\n       Alle Achtung denen, die einen Neuanfang wagen, aber sie sollten bei aller Begeisterung bedenken: sie bleiben immer sie selbst, Vor sich selber davon laufen geht nicht.\\n      </p>\\n     </div>\\n    </div>\\n   </li>\\n   <!-- #comment-## -->\\n  </ul>\\n </div>\\n</div>\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disqus_df[disqus_df['comments'].apply(lambda x: len(x)>32)]['comments'][2]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97db6283-759d-4564-be6f-12a13a595d2d",
   "metadata": {},
   "source": [
    "import gc # For freeing up RAM\n",
    "import pandas as pd\n",
    "\n",
    "def filter_df_content(file, chunksize):\n",
    "    list_disqus = [] # Appending to list before conferting to dataframe because it is computationally cheaper\n",
    "    \n",
    "    # Loops through the CSV in chunks and saves those with disqus.js in the content column \n",
    "    for chunk in pd.read_csv(file, iterator=True, chunksize=chunksize):\n",
    "        # Add all rows with \"disqus.js\" in the content column to list_disqus\n",
    "        list_disqus.append(chunk[chunk.content.str.contains('(?i)disqus', regex= True, na=False)])\n",
    "        # chunk_offset = chunk_offset + chunksize\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    disqus_df = pd.concat(list_disqus, ignore_index=True) # Convert the list to a pandas dataframe\n",
    "    # Changing crawl_data to date object\n",
    "    #disqus_df['crawl_date']= pd.to_datetime(disqus_df['crawl_date'],format='%Y%m%d')\n",
    "\n",
    "    return disqus_df\n",
    "\n",
    "\n",
    "disqus_df = filter_df_content(\"html-file-information.csv\", 40000)\n",
    "disqus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f3f09-6c8a-4de6-83eb-faf0d938f7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
