{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348e893b-e773-4e96-956d-74e6fd110ef4",
   "metadata": {},
   "source": [
    "# Download an archive\n",
    "The script below makes a data folder with a subfolder for the dataset. Dataset option are \"World\", \"German\" or \"Dutch\", select these by altering the dataset variable. The script wil skip files that are already present on the system. If you want to redownload the files first remove the old files manually.  \n",
    "If the download fails due to connection errors, rerun the script and will will notice that he file did not decompress and try again.  \n",
    "Note: The html-file-information.csv can be very large and take several hours and fail. Consider putting the code below code in a loop if you want to run it over night, if the download fails it will try again and if successfull it will say it already has the desired file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77006d6e-6e30-43b3-9353-37de21a7e004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "mkdir: cannot create directory ‘data/World’: File exists\n",
      "domain-frequency.csv already exists\n",
      "domain-frequency.csv already exists\n",
      "css-file-information.csv already exists\n",
      "js-file-information.csv already exists\n",
      "html-file-information.csv is not present\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 6594M    0 6594M    0     0   318k      0 --:--:--  5:53:49 --:--:--  203k-:--  0:06:57 --:--:--  352k 0:11:49 --:--:--  200k:--:--  511k6k519k-:--:--  0:27:20 --:--:--  269k  343k-:--  0:40:46 --:--:--  410k 0:43:06 --:--:--  283k0M    0     0   380k      0 --:--:--  0:43:34 --:--:--  368k-:--:--  283k54:19 --:--:--  368k  1:02:14 --:--:-- 1212k--  377k--:--  1:07:29 --:--:--  646k--:--  1:09:09 --:--:--  301k--  1:11:51 --:--:--  326k  321k:12:13 --:--:--  361k   0 --:--:--  1:28:32 --:--:--  715k--:--:--  1:30:33 --:--:--  327k:--  582k    0 --:--:--  1:42:00 --:--:--  289k:--:--  669k  322k--  2:22:59 --:--:--  227k      0 --:--:--  2:29:27 --:--:--  381k242k--:--  2:34:08 --:--:--  287k:--  2:34:44 --:--:--  405k-:--  238k8k-:--:--  182k 3:06:46 --:--:--  310k  0     0   350k      0 --:--:--  3:18:19 --:--:--  250k 3:26:10 --:--:--  249k:--  3:32:17 --:--:--  377k:34:20 --:--:--  242k --:--:--  3:36:17 --:--:--  272k  3:50:45 --:--:--  213k:--:--  237k-:--:--  4:05:01 --:--:--  172k--:--  144k   330k      0 --:--:--  4:28:55 --:--:--  248k332k  0 --:--:--  4:35:07 --:--:--  275kk--  218k-:--  217k 301k:--:--  285k4k6k   0 --:--:--  5:19:06 --:--:--  218k--:--  171k:--:--  271k\n",
      "html-file-information.csv has been downloaded\n"
     ]
    }
   ],
   "source": [
    "# Dataset option are \"World\", \"German\" or \"Dutch\"\n",
    "Dataset = \"World\"\n",
    "\n",
    "!mkdir data\n",
    "!mkdir data/{Dataset}\n",
    "\n",
    "import os.path\n",
    "\n",
    "if Dataset == \"German\":\n",
    "    if not os.path.exists(\"data/German/domain-frequency.csv\"):\n",
    "        print(\"domain-frequency.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/DomainFrequencyExtraction/domain-frequency.csv.gz?access=VRQ4COI5RFEB6XTJZNQTBRLEZTTHJERL\" --output data/German/domain-frequency.csv.gz\n",
    "        print(\"domain-frequency.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/German/domain-graph.csv\"):\n",
    "        print(\"domain-graph.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/DomainGraphExtraction/domain-graph.csv.gz?access=JKEPGQ6MUC72JQB23IXOC4KOLGJYDSMN\" --output data/German/domain-graph.csv.gz\n",
    "        print(\"domain-graph.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/German/css-file-information.csv\"):\n",
    "        print(\"css-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/css-file-information.csv.gz?access=I2WP4REJA3NOBU3TCAAL3OIGJKNXM46R\" --output data/German/css-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"css-file-information.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/German/js-file-information.csv\"):\n",
    "        print(\"js-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/js-file-information.csv.gz?access=M3QSMFPLEHPZPWZIFSMZ6CT2OO7WYQ4M\" --output data/German/js-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"js-file-information.csv already exists\")\n",
    "        \n",
    "    if not os.path.exists(\"data/German/html-file-information.csv\"):\n",
    "        print(\"html-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-germanNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/html-file-information.csv.gz?access=E3BUHKL5P4Q3TD4LGXZV2AOMERYQ3GWL\" --output data/German/html-file-information.csv.gz\n",
    "        print(\"html-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"html-file-information.csv already exists\")\n",
    "        \n",
    "if Dataset == \"World\":\n",
    "    if not os.path.exists(\"data/World/domain-frequency.csv\"):\n",
    "        print(\"domain-frequency.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/DomainFrequencyExtraction/domain-frequency.csv.gz?access=SMSQY3G6IGKGRWVLGCWMA7DMCHBCKQ4K\" --output data/World/domain-frequency.csv.gz\n",
    "        print(\"domain-frequency.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/World/domain-graph.csv\"):\n",
    "        print(\"domain-graph.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/DomainGraphExtraction/domain-graph.csv.gz?sample=true&access=BZTA7LW5LUNMPGKWQMLKAUAXBV2E2AEC\" --output data/World/domain-graph.csv.gz\n",
    "        print(\"domain-graph.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/World/css-file-information.csv\"):\n",
    "        print(\"css-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/css-file-information.csv.gz?access=UZ4PWVRXXWPF53BHM7TTYNK24P7YAIXQ\" --output data/World/css-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"css-file-information.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/World/js-file-information.csv\"):\n",
    "        print(\"js-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/js-file-information.csv.gz?access=54X7IV7QOOAJWGHKPRGDI7HIR6W6GKPI\" --output data/World/js-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"js-file-information.csv already exists\")\n",
    "    \n",
    "    if not os.path.exists(\"data/World/html-file-information.csv\"):\n",
    "        print(\"html-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-worldNews_1st-in-month_1-hop_unique-EXTRACTION-20210818232425/TextFilesInformationExtraction/html-file-information.csv.gz?access=JQEDT3PRXOS6OXZ7LA5EC55HQJVUASDX\" --output data/World/html-file-information.csv.gz\n",
    "        print(\"html-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"html-file-information.csv already exists\")\n",
    "\n",
    "if Dataset == \"Dutch\":\n",
    "    if not os.path.exists(\"data/Dutch/domain-frequency.csv\"):\n",
    "        print(\"domain-frequency.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/DomainFrequencyExtraction/domain-frequency.csv.gz?access=QUQTRHVNIDKXN62S4D62XFHCFI7VDSH7\" --output data/Dutch/domain-frequency.csv.gz\n",
    "        print(\"domain-frequency.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/Dutch/domain-graph.csv\"):\n",
    "        print(\"domain-graph.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/DomainGraphExtraction/domain-graph.csv.gz?access=JUNESOMDTBYDNCCGVJI5MJHW45KHAIJZ\" --output data/Dutch/domain-graph.csv.gz\n",
    "        print(\"domain-graph.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"domain-frequency.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/Dutch/css-file-information.csv\"):\n",
    "        print(\"css-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/TextFilesInformationExtraction/css-file-information.csv.gz?access=IGBKSGXJBL3L4IJ2LXS5NPJTOY77JCT5\" --output data/Dutch/css-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"css-file-information.csv already exists\")\n",
    "\n",
    "    if not os.path.exists(\"data/Dutch/js-file-information.csv\"):\n",
    "        print(\"js-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/TextFilesInformationExtraction/js-file-information.csv.gz?access=RPZKGWUTWJBIRKOKKHGECB7W4OHUATMO\" --output data/Dutch/js-file-information.csv.gz\n",
    "        print(\"css-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"js-file-information.csv already exists\")\n",
    "    \n",
    "    if not os.path.exists(\"data/Dutch/html-file-information.csv\"):\n",
    "        print(\"html-file-information.csv is not present\")\n",
    "        !curl \"https://webdata.archive-it.org/ait/arch:cohort.helmond/research_services/download/SPECIAL-dutchNews_1st-in-month_1-hop_unique-EXTRACTION-20210916172606/TextFilesInformationExtraction/html-file-information.csv.gz?access=52ZOSNQQMFWKW42WMQPNMGKYLEVUQHUC\" --output data/Dutch/html-file-information.csv.gz\n",
    "        print(\"html-file-information.csv has been downloaded\")\n",
    "    else:\n",
    "        print(\"html-file-information.csv already exists\")\n",
    "\n",
    "!find data/{Dataset} -name '*.gz' -exec gunzip {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9143d",
   "metadata": {},
   "source": [
    "# Apply patch to fix pandas memory leak issue\n",
    "\n",
    "The code bellow comes from https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442  \n",
    "It solves a memory leak issue caused by pandas using a library that does not release memory correctly causing memory to fill up over time and eventually crash if RAM is exceded.\n",
    "\n",
    "This may not be a problem for smaller datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "485b627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying monkeypatch for pd.DataFrame.__del__\n"
     ]
    }
   ],
   "source": [
    "# monkeypatches.py\n",
    "\n",
    "# Solving memory leak problem in pandas\n",
    "# https://github.com/pandas-dev/pandas/issues/2659#issuecomment-12021083\n",
    "import sys\n",
    "import pandas as pd\n",
    "from ctypes import cdll, CDLL\n",
    "try:\n",
    "    cdll.LoadLibrary(\"libc.so.6\")\n",
    "    libc = CDLL(\"libc.so.6\")\n",
    "    libc.malloc_trim(0)\n",
    "except (OSError, AttributeError):\n",
    "    libc = None\n",
    "\n",
    "__old_del = getattr(pd.DataFrame, '__del__', None)\n",
    "\n",
    "def __new_del(self):\n",
    "    if __old_del:\n",
    "        __old_del(self)\n",
    "    libc.malloc_trim(0)\n",
    "\n",
    "if libc:\n",
    "    print('Applying monkeypatch for pd.DataFrame.__del__', file=sys.stderr)\n",
    "    pd.DataFrame.__del__ = __new_del\n",
    "else:\n",
    "    print('Skipping monkeypatch for pd.DataFrame.__del__: libc or malloc_trim() not found', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7a4bf-531d-40c9-bc19-2e28234c50c9",
   "metadata": {},
   "source": [
    "# Restrict domain list\n",
    "\n",
    "The code below filters out unwanted domains from the dataset an creates a new CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fb856-cfc3-49b5-9d25-8376ce3e78ef",
   "metadata": {},
   "source": [
    "The desired domains are extracted from a column labled \"URL\" in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f6e5fc7-5cd9-40e1-a8de-7ab0102dc516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        bild.de\n",
       "1                         waz.de\n",
       "2                        zeit.de\n",
       "3                sueddeutsche.de\n",
       "4                        welt.de\n",
       "5                   rp-online.de\n",
       "6                        faz.net\n",
       "7       augsburger-allgemeine.de\n",
       "8           allgaeuer-zeitung.de\n",
       "9                 freiepresse.de\n",
       "10                     merkur.de\n",
       "11                        lvz.de\n",
       "12                         mz.de\n",
       "13                         wn.de\n",
       "14                        hna.de\n",
       "15                 rheinpfalz.de\n",
       "16                volksstimme.de\n",
       "17              handelsblatt.com\n",
       "18                        taz.de\n",
       "19                 nd-aktuell.de\n",
       "20              jungefreiheit.de\n",
       "21                    freitag.de\n",
       "22              das-parlament.de\n",
       "23       juedische-allgemeine.de\n",
       "24                         fr.de\n",
       "25                 abendblatt.de\n",
       "26                 morgenpost.de\n",
       "27               tagesspiegel.de\n",
       "28                  bz-berlin.de\n",
       "29                        ftd.de\n",
       "30                       ksta.de\n",
       "31        stuttgarter-zeitung.de\n",
       "32           badische-zeitung.de\n",
       "33                         tz.de\n",
       "34                       mopo.de\n",
       "35           berliner-zeitung.de\n",
       "36                        haz.de\n",
       "37    stuttgarter-nachrichten.de\n",
       "38                    spiegel.de\n",
       "39            deutschlandfunk.de\n",
       "40                      focus.de\n",
       "41                      stern.de\n",
       "42                        zdf.de\n",
       "43                        wdr.de\n",
       "44                        ndr.de\n",
       "45                         br.de\n",
       "46                        swr.de\n",
       "47                   mainpost.de\n",
       "48                  nwzonline.de\n",
       "49                       mads.de\n",
       "Name: URL, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_list = pd.read_csv(\"Data/German/Top News German news Websites.csv\", usecols=[\"URL\"])\n",
    "\n",
    "from urllib import parse\n",
    "\n",
    "def to_domain(full_url):\n",
    "    netloc = parse.urlparse(full_url).netloc\n",
    "    if netloc.startswith('www.'):\n",
    "        netloc = netloc[4:]\n",
    "    return netloc\n",
    "\n",
    "domains_in_scope = domains_list[\"URL\"].apply(to_domain) \n",
    "\n",
    "domains_in_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e5d7c-c88c-4103-976e-e3004eaa114e",
   "metadata": {},
   "source": [
    "The **chunksize** variable determines how much memory is used. A larger chunksize allows for faster computation, but will crash when RAM is full. If the computation crashes try restarting the kernel and/or a lower chunksize. A chunksize of 1000 should be fine for most machines. On our machine memory is not an issue, but still it is something to consider when running the same code on different machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00720d75-4845-4427-988c-c7f742722384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc # For freeing up RAM\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def filter_df_domains(input_file, output_file, chunksize):\n",
    "    output = open(output_file, \"w\")\n",
    "    output.close()\n",
    "    pd.read_csv(input_file, header=None, nrows=1).to_csv(output_file, mode='a', header=False)\n",
    "    \n",
    "    # Loops through the CSV in chunks and saves those with URLs thst match those in the domains_in_scope \n",
    "    for chunk in pd.read_csv(input_file, iterator=True, chunksize=chunksize):\n",
    "        chunk=chunk[chunk.url.apply(to_domain).isin(domains_in_scope)]\n",
    "        chunk.to_csv(output_file, mode='a', header=False)\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "filter_df_domains(\"Data/German/html-file-information.csv\", \"Data/German/inScope.csv\", 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a166ff9",
   "metadata": {},
   "source": [
    "# Extract wepages holding disqus.js in their HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9766d4a",
   "metadata": {},
   "source": [
    "The code bellow Looks through a dataset for any mention of disqus.js, an indicator of the presence of the disqus commenting system. The input for ```filter_df_content``` is the desired dataset file and the chunksize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93892bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc # For freeing up RAM\n",
    "import pandas as pd\n",
    "\n",
    "def filter_df_content_regex(file, regex, chunksize):\n",
    "    list_disqus = [] # Appending to list before conferting to dataframe because it is computationally cheaper\n",
    "    \n",
    "    # Loops through the CSV in chunks and saves those with disqus.js in the content column \n",
    "    for chunk in pd.read_csv(file, iterator=True, chunksize=chunksize):\n",
    "        # Add all rows with \"disqus.js\" in the content column to list_disqus\n",
    "        list_disqus.append(chunk[chunk.content.str.contains(regex, regex= True, na=False)])\n",
    "        # chunk_offset = chunk_offset + chunksize\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    disqus_df = pd.concat(list_disqus, ignore_index=True) # Convert the list to a pandas dataframe\n",
    "\n",
    "    return disqus_df\n",
    "\n",
    "# Options are \"Data/German/html-file-information.csv\" \"Data/World/html-file-information.csv\" and \"Data/Dutch/html-file-information.csv\"\n",
    "# or any other dataset on the system.\n",
    "disqus_df = filter_df_content_regex(\"Data/World/html-file-information.csv\", \"(?i)disqus\\.js\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f575b650-f771-40a7-8d8d-c753bc549595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1916"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(disqus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0170c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1916"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disqus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef06e55",
   "metadata": {},
   "source": [
    "Note that this finds more hits than grep. It seems the csv file is too large for grep. the same query in awk does give the expected number of results\n",
    "\n",
    "```grep -i -n 'disqus\\.js' C/Users/rjans/Desktop/ARCH/Data/disqus.csv```  \n",
    "vs  \n",
    "```awk -e '/disqus\\.js/ {print $0}' C/Users/rjans/Desktop/ARCH/Data/html-file-information-German.csv```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd05ef",
   "metadata": {},
   "source": [
    "Save the results in a seperate CSV (only needs to be done once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccb954b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disqus_df.to_csv(\"disqus.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a04515",
   "metadata": {},
   "source": [
    "Convert **content** column to single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcaa1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in disqus_df.content:\n",
    "    disqus_df.content = disqus_df.content.replace(line, \" \".join(line.splitlines()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412449fe",
   "metadata": {},
   "source": [
    "Save as single line content in a seperate CSV (only needs to be done once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ea37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disqus_df.to_csv(\"disqus_single_line.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250400c",
   "metadata": {},
   "source": [
    "# For the found pages extract the comment structure\n",
    "\n",
    "The code below requires beautifulsoup4. use the commented out import and pip command to install the library in the right place if you do not already beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b652f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00a560",
   "metadata": {},
   "source": [
    "Extract the disqus comments from the div tag with ```id=\"disqus_thread\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fafe251",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "for line in disqus_df.content:\n",
    "    soup = BeautifulSoup(line, 'html.parser')\n",
    "    comments.append(soup.find('div', attrs={'id': 'disqus_thread'}).prettify()) # Makes the comments more human readable\n",
    "    \n",
    "disqus_df['comments'] = comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da28729a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>url</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20160420</td>\n",
       "      <td>https://www.bilanz.de/exklusiv/einzelhandel-wa...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20160408</td>\n",
       "      <td>https://www.bilanz.de/maerkte/wachstum-mit-kun...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170622</td>\n",
       "      <td>http://tussiblog.nwzonline.de/alles-aufgegeben...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170924</td>\n",
       "      <td>http://tussiblog.nwzonline.de/die-drei-magisch...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170113</td>\n",
       "      <td>http://tussiblog.nwzonline.de/die-ungeheure-ma...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20170622</td>\n",
       "      <td>http://tussiblog.nwzonline.de/kaninchen/</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20170924</td>\n",
       "      <td>http://tussiblog.nwzonline.de/lasst-das-wilde-...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20170923</td>\n",
       "      <td>http://tussiblog.nwzonline.de/saudi-women/</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20170129</td>\n",
       "      <td>http://tussiblog.nwzonline.de/saudi-women/</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20170622</td>\n",
       "      <td>http://tussiblog.nwzonline.de/warteschleife/</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20170923</td>\n",
       "      <td>http://tussiblog.nwzonline.de/zehn-dinge-von-d...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20170129</td>\n",
       "      <td>http://tussiblog.nwzonline.de/zehn-dinge-von-d...</td>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n&lt;/div&gt;\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    crawl_date                                                url  \\\n",
       "0     20160420  https://www.bilanz.de/exklusiv/einzelhandel-wa...   \n",
       "1     20160408  https://www.bilanz.de/maerkte/wachstum-mit-kun...   \n",
       "2     20170622  http://tussiblog.nwzonline.de/alles-aufgegeben...   \n",
       "3     20170924  http://tussiblog.nwzonline.de/die-drei-magisch...   \n",
       "4     20170113  http://tussiblog.nwzonline.de/die-ungeheure-ma...   \n",
       "5     20170622           http://tussiblog.nwzonline.de/kaninchen/   \n",
       "6     20170924  http://tussiblog.nwzonline.de/lasst-das-wilde-...   \n",
       "7     20170923         http://tussiblog.nwzonline.de/saudi-women/   \n",
       "8     20170129         http://tussiblog.nwzonline.de/saudi-women/   \n",
       "9     20170622       http://tussiblog.nwzonline.de/warteschleife/   \n",
       "10    20170923  http://tussiblog.nwzonline.de/zehn-dinge-von-d...   \n",
       "11    20170129  http://tussiblog.nwzonline.de/zehn-dinge-von-d...   \n",
       "\n",
       "                                             comments  \n",
       "0                  <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "1                  <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "2   <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...  \n",
       "3   <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...  \n",
       "4                  <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "5                  <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "6   <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...  \n",
       "7                  <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "8                  <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "9   <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...  \n",
       "10                 <div id=\"disqus_thread\">\\n</div>\\n  \n",
       "11                 <div id=\"disqus_thread\">\\n</div>\\n  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_colwidth', 500) \n",
    "disqus_df[['crawl_date','url','comments']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea198f9-e4da-42e0-b35f-55d73ca7065e",
   "metadata": {},
   "source": [
    "# Add an internet archive URL to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7eac9c",
   "metadata": {},
   "source": [
    "Use the ``crawl_date`` and ``url`` columns to estimate the Internet Archive link. The CSV only provides the date of the crawl, not the time, so the resulting link may be for the wrong capure if more than one capture is available on that date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb52f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ia_url(df):\n",
    "    IA_url = []\n",
    "    for index, row in df.iterrows():\n",
    "        url = \"https://web.archive.org/web/\" + str(row['crawl_date']) + \"/\" + row[\"url\"]\n",
    "        IA_url.append(url)\n",
    "    disqus_df['IA_url'] = IA_url\n",
    "\n",
    "add_ia_url(disqus_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948144a",
   "metadata": {},
   "source": [
    "Filter out comment fields that are so short that they do net contain comments (based on the length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dcaebce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>IA_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20170622/http://tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20170924/http://tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20170924/http://tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;div id=\"disqus_thread\"&gt;\\n &lt;div id=\"dsq-conten...</td>\n",
       "      <td>https://web.archive.org/web/20170622/http://tu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  \\\n",
       "2  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "3  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "6  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "9  <div id=\"disqus_thread\">\\n <div id=\"dsq-conten...   \n",
       "\n",
       "                                              IA_url  \n",
       "2  https://web.archive.org/web/20170622/http://tu...  \n",
       "3  https://web.archive.org/web/20170924/http://tu...  \n",
       "6  https://web.archive.org/web/20170924/http://tu...  \n",
       "9  https://web.archive.org/web/20170622/http://tu...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disqus_df[disqus_df['comments'].apply(lambda x: len(x)>32)][['comments','IA_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3561edde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div id=\"disqus_thread\">\\n <div id=\"dsq-content\">\\n  <ul id=\"dsq-comments\">\\n   <li class=\"comment even thread-even depth-1\" id=\"dsq-comment-384\">\\n    <div class=\"dsq-comment-header\" id=\"dsq-comment-header-384\">\\n     <cite id=\"dsq-cite-384\">\\n      <span id=\"dsq-author-user-384\">\\n       Bolko Schröder\\n      </span>\\n     </cite>\\n    </div>\\n    <div class=\"dsq-comment-body\" id=\"dsq-comment-body-384\">\\n     <div class=\"dsq-comment-message\" id=\"dsq-comment-message-384\">\\n      <p>\\n       Alle Achtung denen, die einen Neuanfang wagen, aber sie sollten bei aller Begeisterung bedenken: sie bleiben immer sie selbst, Vor sich selber davon laufen geht nicht.\\n      </p>\\n     </div>\\n    </div>\\n   </li>\\n   <!-- #comment-## -->\\n  </ul>\\n </div>\\n</div>\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disqus_df[disqus_df['comments'].apply(lambda x: len(x)>32)]['comments'][2]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97db6283-759d-4564-be6f-12a13a595d2d",
   "metadata": {},
   "source": [
    "import gc # For freeing up RAM\n",
    "import pandas as pd\n",
    "\n",
    "def filter_df_content(file, chunksize):\n",
    "    list_disqus = [] # Appending to list before conferting to dataframe because it is computationally cheaper\n",
    "    \n",
    "    # Loops through the CSV in chunks and saves those with disqus.js in the content column \n",
    "    for chunk in pd.read_csv(file, iterator=True, chunksize=chunksize):\n",
    "        # Add all rows with \"disqus.js\" in the content column to list_disqus\n",
    "        list_disqus.append(chunk[chunk.content.str.contains('(?i)disqus', regex= True, na=False)])\n",
    "        # chunk_offset = chunk_offset + chunksize\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    disqus_df = pd.concat(list_disqus, ignore_index=True) # Convert the list to a pandas dataframe\n",
    "    # Changing crawl_data to date object\n",
    "    #disqus_df['crawl_date']= pd.to_datetime(disqus_df['crawl_date'],format='%Y%m%d')\n",
    "\n",
    "    return disqus_df\n",
    "\n",
    "\n",
    "disqus_df = filter_df_content(\"html-file-information.csv\", 40000)\n",
    "disqus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4abbb27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "      <th>extension</th>\n",
       "      <th>mime_type_web_server</th>\n",
       "      <th>mime_type_tika</th>\n",
       "      <th>md5</th>\n",
       "      <th>sha1</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [0, crawl_date, url, filename, extension, mime_type_web_server, mime_type_tika, md5, sha1, content]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc # For freeing up RAM\n",
    "import pandas as pd\n",
    "\n",
    "def filter_df_content(file, chunksize):\n",
    "    list_disqus = [] # Appending to list before conferting to dataframe because it is computationally cheaper\n",
    "    \n",
    "    # Loops through the CSV in chunks and saves those with disqus.js in the content column \n",
    "    for chunk in pd.read_csv(file, iterator=True, chunksize=chunksize):\n",
    "        # Add all rows with \"disqus.js\" in the content column to list_disqus\n",
    "        list_disqus.append(chunk[chunk.content.str.contains('(?i)disqus\\.js', regex= True, na=False)])\n",
    "        # chunk_offset = chunk_offset + chunksize\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    disqus_df = pd.concat(list_disqus, ignore_index=True) # Convert the list to a pandas dataframe\n",
    "\n",
    "    return disqus_df\n",
    "\n",
    "disqus_df = filter_df_content(\"Data/German/inScope.csv\", 40000)\n",
    "disqus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f3f09-6c8a-4de6-83eb-faf0d938f7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
