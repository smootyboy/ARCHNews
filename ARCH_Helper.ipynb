{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eda7642-bf46-49c7-9d65-79e6e332cbd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420b75e-7351-40a1-918e-fe311f937c25",
   "metadata": {
    "tags": []
   },
   "source": [
    "The required packages to run this notebook.  \n",
    "If any are missing use ``!{sys.executable} -m pip install <package>`` to install the package in the notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "169543b9-0c08-4d9f-ac3c-e1521e24f7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import gc \n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "from ctypes import cdll, CDLL\n",
    "from urllib import parse\n",
    "#!{sys.executable} -m pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce750ad8-5eb7-41af-8eef-4218134778e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17e906f3-2b71-40be-915b-fb5887d4b760",
   "metadata": {
    "tags": []
   },
   "source": [
    "# General helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42613bd1-558e-43e6-af9b-4a1d12dc67e4",
   "metadata": {},
   "source": [
    "## Apply patch to fix pandas memory leak issue\n",
    "\n",
    "The code bellow comes from https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442  \n",
    "It solves a memory leak issue caused by pandas using a library that does not release memory correctly causing memory to fill up over time and eventually crash if RAM is exceded.\n",
    "\n",
    "This may not be a problem for smaller datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d69811-6ceb-44a0-be10-1bd8c54945f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying monkeypatch for pd.DataFrame.__del__\n"
     ]
    }
   ],
   "source": [
    "# monkeypatches.py\n",
    "\n",
    "# Solving memory leak problem in pandas\n",
    "# https://github.com/pandas-dev/pandas/issues/2659#issuecomment-12021083\n",
    "try:\n",
    "    cdll.LoadLibrary(\"libc.so.6\")\n",
    "    libc = CDLL(\"libc.so.6\")\n",
    "    libc.malloc_trim(0)\n",
    "except (OSError, AttributeError):\n",
    "    libc = None\n",
    "\n",
    "__old_del = getattr(pd.DataFrame, '__del__', None)\n",
    "\n",
    "def __new_del(self):\n",
    "    if __old_del:\n",
    "        __old_del(self)\n",
    "    libc.malloc_trim(0)\n",
    "\n",
    "if libc:\n",
    "    print('Applying monkeypatch for pd.DataFrame.__del__', file=sys.stderr)\n",
    "    pd.DataFrame.__del__ = __new_del\n",
    "else:\n",
    "    print('Skipping monkeypatch for pd.DataFrame.__del__: libc or malloc_trim() not found', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddcd265-3633-49aa-9e8f-66c2979d8551",
   "metadata": {},
   "source": [
    "## Convert a full url to a domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55dfb64-660a-4a40-9482-911fafd314f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_domain(full_url):\n",
    "    if(parse.urlparse(full_url).netloc == \"\"):\n",
    "        full_url = \"//\" + full_url\n",
    "    netloc = parse.urlparse(full_url).netloc\n",
    "    if netloc.startswith('www.'):\n",
    "        netloc = netloc[4:]\n",
    "    return netloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27029e7c-14fd-4b36-b4b0-c6a8215aa198",
   "metadata": {},
   "source": [
    "## Filter dataframe with regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f98b56-5b54-4d84-864b-c50e2e7ad87f",
   "metadata": {},
   "source": [
    "The **chunksize** variable determines how much memory is used. A larger chunksize allows for faster computation, but will crash when RAM is full. If the computation crashes try restarting the kernel and/or a lower chunksize. A chunksize of 1000 should be fine for most machines. On our machine memory is not an issue, but still it is something to consider when running the same code on different machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62cf8d8a-d434-451c-ae3f-4d224d9be14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_csv_content_regex(input_file, regex, chunksize, output_file=None):\n",
    "    if (output_file is None):\n",
    "        list_regex = [] # Appending to list before conferting to dataframe because it is computationally cheaper\n",
    "        \n",
    "        # Loops through the CSV in chunks and saves those that match the regex in the content column \n",
    "        for chunk in pd.read_csv(input_file, iterator=True, chunksize=chunksize):\n",
    "            # Add all rows with the regex in the content column to list_regex\n",
    "            list_regex.append(chunk[chunk.content.str.contains(regex, regex= True, na=False)])\n",
    "            # chunk_offset = chunk_offset + chunksize\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "        filtered_df = pd.concat(list_regex, ignore_index=True) # Convert the list to a pandas dataframe\n",
    "\n",
    "        return filtered_df\n",
    "    \n",
    "    else:\n",
    "        output = open(output_file, \"w\")\n",
    "        output.close()\n",
    "        pd.read_csv(input_file, header=None, nrows=1).to_csv(output_file, mode='a', header=False, index=False)\n",
    "\n",
    "        # Loops through the CSV in chunks and saves those that match the regex in the content column \n",
    "        for chunk in pd.read_csv(input_file, iterator=True, chunksize=chunksize):\n",
    "            # Add all rows with the regex in the content column to list_regex\n",
    "            chunk = chunk[chunk.content.str.contains(regex, regex= True, na=False)]\n",
    "            chunk.to_csv(output_file, mode='a', header=False, index=False)\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be626032-a4b0-4cc3-888f-c2df18171fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_content_regex(input_df, regex, output_file=None):\n",
    "    if (output_file is None):\n",
    "        return input_df[input_df.content.str.contains(regex, regex= True, na=False)]\n",
    "    else:\n",
    "        save_dataframe(input_df[input_df.content.str.contains(regex, regex= True, na=False)], output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711100ca-d705-4278-8029-08d89af6d4b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filter dataframe by date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b9742-c3ad-4062-a97d-feb94e9ae5c6",
   "metadata": {},
   "source": [
    "Filer csv or dataframe by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "142d25cb-e2a3-4639-a2eb-4cbd2671e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_csv_date(input_file, begin_date, end_date , chunksize, output_file=None):\n",
    "    if (output_file is None):\n",
    "        list = [] # Appending to list before conferting to dataframe because it is computationally cheaper\n",
    "\n",
    "        # Loops through the CSV in chunks and saves those that match the given timeframe\n",
    "        for chunk in pd.read_csv(input_file, iterator=True, chunksize=chunksize):\n",
    "            # Add all rows that match the given time frame\n",
    "            list.append(chunk[(chunk['crawl_date'] >= begin_date) & (chunk['crawl_date'] <= end_date)])\n",
    "            # chunk_offset = chunk_offset + chunksize\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "        filtered_df = pd.concat(list, ignore_index=True) # Convert the list to a pandas dataframe\n",
    "\n",
    "        return filtered_df\n",
    "    else:\n",
    "        output = open(output_file, \"w\")\n",
    "        output.close()\n",
    "        pd.read_csv(input_file, header=None, nrows=1).to_csv(output_file, mode='a', header=False, index=False)\n",
    "\n",
    "        # Loops through the CSV in chunks and saves those that match that match the given timeframe\n",
    "        for chunk in pd.read_csv(input_file, iterator=True, chunksize=chunksize):\n",
    "            # Add all rows with the regex in the content column to list_regex\n",
    "            chunk = chunk[(chunk['crawl_date'] >= begin_date) & (chunk['crawl_date'] <= end_date)]\n",
    "            chunk.to_csv(output_file, mode='a', header=False, index=False)\n",
    "            del chunk\n",
    "            gc.collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb5a6d24-b842-4ef3-aea8-84f58e78a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_date(input_df, begin_date, end_date, output_file=None):\n",
    "    if (output_file is None):\n",
    "        return input_df[(input_df['crawl_date'] >= begin_date) & (input_df['crawl_date'] <= end_date)]\n",
    "    else:\n",
    "        save_dataframe(input_df[(input_df['crawl_date'] >= begin_date) & (input_df['crawl_date'] <= end_date)], output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbff49d-d3ed-4143-86fb-aac5d6125008",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Add an Internet archive Wayback Machine link to a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae48b91-aa3f-4c68-8736-a1ea2857f1a7",
   "metadata": {},
   "source": [
    "Use the ``crawl_date`` and ``url`` columns to estimate the Internet Archive link. The CSV only provides the date of the crawl, not the time, so the resulting link may be for the wrong capure if more than one capture is available on that date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7968fb2-8ab8-45d4-9ee6-4e2e640d0d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ia_url(df):\n",
    "    IA_url = []\n",
    "    for index, row in df.iterrows():\n",
    "        url = \"https://web.archive.org/web/\" + str(row['crawl_date']) + \"/\" + row[\"url\"]\n",
    "        IA_url.append(url)\n",
    "    df['IA_url'] = IA_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fa2fa-2e52-427f-aabc-6ab8daf7584c",
   "metadata": {},
   "source": [
    "## Display a specific comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d7d6613-929c-48e4-861f-91a9f6fee7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comment(dataframe, index):\n",
    "    print(dataframe['comments'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf440c9-61f7-4466-9143-047cde6108ff",
   "metadata": {},
   "source": [
    "## Display number of entries per domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b52b314-ca48-4b01-81fd-3e0718edfa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_per_domain(df):\n",
    "    return df['url'].apply(to_domain).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1352ca4-159c-4845-8937-c502d427e365",
   "metadata": {},
   "source": [
    "## Intersection of two dataframes\n",
    "\n",
    "Returns a dataframe with the intersection (overlap) of two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71701de-b200-4b01-90f6-4ad51e156646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_intersection(df1,df2):\n",
    "    return pd.merge(df1, df2, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002eb951-7d76-49c0-aa38-4567a6ca2d3b",
   "metadata": {},
   "source": [
    "## Dataframe difference\n",
    "\n",
    "Returns a dataframe with all entries in each dataframe but not those in both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ffca9a7-c531-4d19-b6cc-04d6cec3371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_difference(df1,df2):\n",
    "    return pd.concat([df1,df2]).drop_duplicates(keep=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb44129-d040-4eef-bcdb-91730e22237c",
   "metadata": {},
   "source": [
    "## Dataframe complement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c242e-cc18-4616-a22d-7a39c0ca6da6",
   "metadata": {},
   "source": [
    "Returns the relative complement of df1 in df2. Meaning all elements in df2 that are not in df1. \n",
    "For CSV files it takes columns as a list input to reduce overall size and computstion time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08feaa6b-2e9d-48e8-a763-51b00d7f2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_complement(csv1, csv2, columns, output_file, chunksize):\n",
    "    output = open(output_file, \"w\")\n",
    "    dw = csv.DictWriter(output, delimiter=',', fieldnames=columns)\n",
    "    dw.writeheader()\n",
    "    output.close()\n",
    "    \n",
    "    for chunk2 in pd.read_csv(csv2, iterator=True, usecols=columns, chunksize=round(chunksize/2)):\n",
    "        for chunk1 in pd.read_csv(csv1, iterator=True, usecols=columns, chunksize=round(chunksize/2)):\n",
    "            chunk2 = dataframe_complement(chunk1,chunk2)\n",
    "            del chunk1\n",
    "            gc.collect()\n",
    "        chunk2.to_csv(output_file, mode='a', header=False, index=False)\n",
    "        del chunk2\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3fb0634-3ebc-48e2-ad67-541a28ae4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_complement(df1,df2):\n",
    "    return pd.concat([df2, df1, df1]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127515a-f0f1-4f78-83b0-0b9c784632f1",
   "metadata": {},
   "source": [
    "## Restrict domain list\n",
    "\n",
    "The code below filters out unwanted domains from the dataset an creates a new CSV file.  \n",
    "The ``domains_in_scope`` input variable is a list of the desired domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64469382-0e33-4f97-b898-c8f0d9f80184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_csv_domains(input_file, output_file, domains_in_scope, chunksize):\n",
    "    output = open(output_file, \"w\")\n",
    "    output.close()\n",
    "    pd.read_csv(input_file, header=None, nrows=1).to_csv(output_file, mode='a', header=False)\n",
    "    \n",
    "    # Loops through the CSV in chunks and saves those with URLs that match those in the domains_in_scope \n",
    "    for chunk in pd.read_csv(input_file, iterator=True, chunksize=chunksize):\n",
    "        chunk=chunk[chunk.url.apply(to_domain).isin(domains_in_scope)]\n",
    "        chunk.to_csv(output_file, mode='a', header=False)\n",
    "        del chunk\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e588da29-092e-41fa-9406-1d455b000e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe_domains(input_df, domains_in_scope):\n",
    "    return input_df[input_df.url.apply(to_domain).isin(domains_in_scope)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e1255e-862e-40af-a6b4-7d826562c3d5",
   "metadata": {},
   "source": [
    "## Save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89458e82-9e4c-441e-a9b9-c62e4ce2a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df, output_file):\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b1dc8-a11f-4ac5-b314-1f22689731d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71495075-185c-4b03-bc45-f3bfee994626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
